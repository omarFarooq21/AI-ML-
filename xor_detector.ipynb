{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNnjQuUPI6uk3s/754Wr/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omarFarooq21/AI-ML-/blob/main/xor_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usxRH7ZjS6GE",
        "outputId": "58bde172-1f4d-43b4-c9c6-16222e720974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current_Loss:  0.7126742416245981\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.33796559, -0.94396909],\n",
              "        [ 0.44555933, -0.94879638]]),\n",
              " array([[-0.00162611,  0.        ]]),\n",
              " array([[-0.47575001,  0.46085631],\n",
              "        [-0.53644775, -1.21953066]]),\n",
              " array([[0.        , 0.00303126]]),\n",
              " array([[-1.13334265,  1.79140554]]),\n",
              " array([[-0.00205836]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# This is my first attempt at implementeing an ANN from scratch.\n",
        "# Key takeaways: Do not use ReLU at the output layer,\n",
        "#                Sigmoid/Softmax work fine for multi class classification at the output layer\n",
        "#                Use Xavier Initialization and do not rely on numpy gaussian init\n",
        "\n",
        "# Next steps: Explore optimizers and tanh/sigmoid in hidden layer and go through the book after exams...\n",
        "# Hopefully build a more complex neural network to solve a semi-real world problem\n",
        "import numpy as np\n",
        "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "labels = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "def xavier_init(size):\n",
        "    return np.random.randn(*size) * np.sqrt(1 / size[0])\n",
        "\n",
        "def model():\n",
        "    # weights_1 = np.random.rand(2,2)\n",
        "    # bias_1 = np.random.rand(1,1)\n",
        "    # weights_2 = np.random.rand(2,2)\n",
        "    # bias_2 = np.random.rand(1,1)\n",
        "    # weights_output = np.random.rand(1, 2)\n",
        "    # bias_output = np.random.rand(1,1)\n",
        "    # weights_1 = np.random.randn(2,2)\n",
        "    # bias_1 = np.random.randn(1,2)\n",
        "    # weights_2 = np.random.randn(2,2)\n",
        "    # bias_2 = np.random.randn(1,2)\n",
        "    # weights_output = np.random.randn(1,2)\n",
        "    # bias_output = np.random.randn(1,1)\n",
        "\n",
        "    weights_1 = xavier_init((2,2))\n",
        "    bias_1 = np.zeros((1,2))\n",
        "    weights_2 = xavier_init((2,2))\n",
        "    bias_2 = np.zeros((1,2))\n",
        "    weights_output = xavier_init((1,2))\n",
        "    bias_output = np.zeros((1,1))\n",
        "\n",
        "    return weights_1, bias_1, weights_2, bias_2, weights_output, bias_output\n",
        "\n",
        "def ReLU(x):\n",
        "    return(np.maximum(0,x))\n",
        "\n",
        "def MSE_Loss(ypred, ytrue):\n",
        "    return np.sum(1/2 * (ypred - ytrue)**2)\n",
        "\n",
        "def binary_cross_entropy_loss(y_pred, y_true):\n",
        "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def forward_pass(list_params, inputs):\n",
        "    weights_1, bias_1, weights_2, bias_2, weights_output, bias_output = list_params\n",
        "    # print(\"Destructured successfully\")\n",
        "    z1 = np.dot(inputs, weights_1) + bias_1\n",
        "    a1 = ReLU(z1)\n",
        "    z2 = np.dot(a1, weights_2) + bias_2\n",
        "    a2 = ReLU(z2)\n",
        "    # print('shape a2: ', a2.shape)\n",
        "    # print('shape weights: ', weights_output.shape)\n",
        "    z3 = np.dot(a2, weights_output.T) + bias_output\n",
        "    # output = ReLU(z3)\n",
        "    output = sigmoid(z3)\n",
        "    # print(\"output: \", output)\n",
        "\n",
        "    return output, a2, a1, z1, z2, z3\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def gradient_activation(x):\n",
        "    return (x > 0).astype(int)\n",
        "\n",
        "def gradient_sigmoid(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "# MSE and BCE derivative\n",
        "def derivative_loss(output, labels):\n",
        "    # Assuming we are using Mean Squared Loss...\n",
        "    return output - labels\n",
        "\n",
        "## ---\n",
        "\n",
        "def compute_gradients_and_update(outputs, list_params, inputs, labels, allowed_print):\n",
        "    output, a2, a1, z1, z2, z3 = outputs\n",
        "    weights_1, bias_1, weights_2, bias_2, weights_output, bias_output = list_params\n",
        "\n",
        "    dL_doutput = derivative_loss(output, labels)\n",
        "    # MSE\n",
        "    # dL_doutput = derivative_loss(output, labels) # Implementation of this function can change (Tanh, sigmoid etc.)\n",
        "    # Swapped ReLU at the output layer for Sigmoid:\n",
        "    doutput_dz3 = gradient_sigmoid(z3)\n",
        "    # RelU\n",
        "    # doutput_dz3 = gradient_activation(z3) # Compute gradient of the ReLU activation function\n",
        "    dL_dz3 = dL_doutput * doutput_dz3  # dL/dy^ * dy^/dz3\n",
        "\n",
        "    #    Output Layer...\n",
        "    dweights_output = np.dot(a2.T, dL_dz3) #dL/dy * dy/dz3 * dz3/dweights_output ...\n",
        "    # print(dweights_output.shape)\n",
        "    dbias_output = np.sum(dL_dz3, axis=0, keepdims=True)\n",
        "\n",
        "    #    Second Hidden Layer...\n",
        "    dL_da2 = np.dot(dL_dz3, weights_output) # weights_output = dz3/da2\n",
        "    da2_dz2 = gradient_activation(z2)\n",
        "    dL_dz2 = dL_da2*da2_dz2\n",
        "    dweights_2 = np.dot(a1.T, dL_dz2) # a1 here because dz2/dweights_2 = a1\n",
        "    # dbias_2 = dL_dz2 * 1              # Equation is written in my notes DLAM..\n",
        "    dbias_2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
        "    #    First Hidden Layer...\n",
        "    dL_da1 = np.dot(dL_dz2, weights_2)\n",
        "    da1_dz1 = gradient_activation(z1)\n",
        "    dL_dz1 = dL_da1 * da1_dz1\n",
        "    dweights_1 = np.dot(inputs.T, dL_dz1)\n",
        "    dbias_1 = np.sum(dL_dz1, axis=0, keepdims=True)      # At this point this becomes intuitive so I didn't write out its equation\n",
        "\n",
        "    lr = 0.03\n",
        "    weights_output -= lr*dweights_output.T\n",
        "    bias_output -= lr*dbias_output\n",
        "    weights_2 -= lr*dweights_2\n",
        "    bias_2 -= lr*dbias_2\n",
        "    weights_1 -= lr*dweights_1\n",
        "    bias_1 -= lr*dbias_1\n",
        "\n",
        "    if(allowed_print):\n",
        "        # print(\"Current_Loss: \", MSE_Loss(output, labels))\n",
        "        print(\"Current_Loss: \", binary_cross_entropy_loss(output, labels))\n",
        "    return weights_1, bias_1, weights_2, bias_2, weights_output, bias_output\n",
        "\n",
        "\n",
        "\n",
        "list_params = model()\n",
        "# print(list_params[0])\n",
        "# output, a2, a1, z1, z2, z3 = forward_pass(list_params, inputs)\n",
        "outputs = forward_pass(list_params, inputs)\n",
        "compute_gradients_and_update(outputs, list_params, inputs, labels, True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5000):\n",
        "    outputs = forward_pass(list_params, inputs)\n",
        "\n",
        "    if(epoch%100 == 0):\n",
        "        list_params = compute_gradients_and_update(outputs, list_params, inputs, labels, True)\n",
        "    else:\n",
        "        list_params = compute_gradients_and_update(outputs, list_params, inputs,labels, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsx0cWv8S9Dr",
        "outputId": "3c6d5a5e-19df-490c-9933-5ef8bc415e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current_Loss:  0.7067898176381358\n",
            "Current_Loss:  0.597772962546051\n",
            "Current_Loss:  0.5671542226008698\n",
            "Current_Loss:  0.544680024481239\n",
            "Current_Loss:  0.5254790268509892\n",
            "Current_Loss:  0.5145486078613186\n",
            "Current_Loss:  0.5072790986010847\n",
            "Current_Loss:  0.5033363646520145\n",
            "Current_Loss:  0.4988476867218368\n",
            "Current_Loss:  0.4967166767456844\n",
            "Current_Loss:  0.4950320120048292\n",
            "Current_Loss:  0.4931062089642466\n",
            "Current_Loss:  0.4920261492803652\n",
            "Current_Loss:  0.49036456310206994\n",
            "Current_Loss:  0.48956588274741536\n",
            "Current_Loss:  0.47958075094874775\n",
            "Current_Loss:  0.44172075755251644\n",
            "Current_Loss:  0.3767077548981707\n",
            "Current_Loss:  0.29409554066474847\n",
            "Current_Loss:  0.22696189263771369\n",
            "Current_Loss:  0.18285787619843352\n",
            "Current_Loss:  0.15511374116746376\n",
            "Current_Loss:  0.13542054193742373\n",
            "Current_Loss:  0.12175666545726917\n",
            "Current_Loss:  0.11079094453525234\n",
            "Current_Loss:  0.10179763360502346\n",
            "Current_Loss:  0.0946795424432097\n",
            "Current_Loss:  0.08860893168911206\n",
            "Current_Loss:  0.08352224402690217\n",
            "Current_Loss:  0.07915075871773634\n",
            "Current_Loss:  0.0752815216966664\n",
            "Current_Loss:  0.07190251028222051\n",
            "Current_Loss:  0.06893016461419543\n",
            "Current_Loss:  0.06619092824387258\n",
            "Current_Loss:  0.06363674407945931\n",
            "Current_Loss:  0.061439040938781164\n",
            "Current_Loss:  0.059437144632990546\n",
            "Current_Loss:  0.05756079130770688\n",
            "Current_Loss:  0.05590951861795815\n",
            "Current_Loss:  0.054256061623302355\n",
            "Current_Loss:  0.05278767200229934\n",
            "Current_Loss:  0.051444320295634685\n",
            "Current_Loss:  0.05013476887590686\n",
            "Current_Loss:  0.04894948171564583\n",
            "Current_Loss:  0.04784640994017136\n",
            "Current_Loss:  0.04682769898916309\n",
            "Current_Loss:  0.04577871668090404\n",
            "Current_Loss:  0.04486324603275274\n",
            "Current_Loss:  0.043972619453718706\n",
            "Current_Loss:  0.043136255467768794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5\n",
        "\n",
        "new_input1 = np.array([[1,1]])\n",
        "new_input2 = np.array([[1,0]])\n",
        "new_input3 = np.array([[0,1]])\n",
        "new_input4 = np.array([[0,0]])\n",
        "\n",
        "predicted_output1, _, _, _, _, _ = forward_pass(list_params, new_input1)\n",
        "predicted_output2, _, _, _, _, _ = forward_pass(list_params, new_input2)\n",
        "predicted_output3, _, _, _, _, _ = forward_pass(list_params, new_input3)\n",
        "predicted_output4, _, _, _, _, _ = forward_pass(list_params, new_input4)\n",
        "\n",
        "predicted_class1 = (predicted_output1 >= threshold).astype(int)\n",
        "predicted_class2 = (predicted_output2 >= threshold).astype(int)\n",
        "predicted_class3 = (predicted_output3 >= threshold).astype(int)\n",
        "predicted_class4 = (predicted_output4 >= threshold).astype(int)\n",
        "\n",
        "print(\"Predicted Class for input [1, 1]:\", predicted_class1)\n",
        "print(\"Predicted Class for input [1, 0]:\", predicted_class2)\n",
        "print(\"Predicted Class for input [0, 1]:\", predicted_class3)\n",
        "print(\"Predicted Class for input [0, 0]:\", predicted_class4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9_bL2SyTBgJ",
        "outputId": "790ab8d7-b268-4144-9dc1-2458a374b8b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class for input [1, 1]: [[0]]\n",
            "Predicted Class for input [1, 0]: [[1]]\n",
            "Predicted Class for input [0, 1]: [[1]]\n",
            "Predicted Class for input [0, 0]: [[0]]\n"
          ]
        }
      ]
    }
  ]
}